{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/markuskunej/air-pollution-thesis/blob/master/VARIMA.ipynb",
      "authorship_tag": "ABX9TyPjirYQjzVMiy6F5CySIohi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markuskunej/air-pollution-thesis/blob/master/VARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfY8pW_uHWaM"
      },
      "outputs": [],
      "source": [
        "#DATA_PATH = \"/content/drive/MyDrive/Air_Pollution_Data/Y&E_60m_last40days\"\n",
        "#!unzip \"/content/drive/MyDrive/Air_Pollution_Data/Y&E_60m_last40days.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/Air_Pollution_Data/Y&E_60m_last40days\"\n",
        "print(os.path.join(DATA_PATH , \"/*.csv\"))\n",
        "all_files = glob.glob(DATA_PATH + '/*.csv')\n",
        "keys = []\n",
        "dfs = []\n",
        "big_df = []\n",
        "for file_name in all_files:\n",
        "  df = pd.read_csv(file_name, parse_dates=[\"Time\"])\n",
        "  variable_name = os.path.basename(file_name).split(\".\")[0]\n",
        "  df[variable_name] = df.mean(axis=1)\n",
        "  #keys.append(variable_name)\n",
        "  df.set_index('Time', inplace=True)\n",
        "  df.info()\n",
        "  df = df.tz_localize(tz='US/Eastern', ambiguous='infer')\n",
        "  df.info()\n",
        "  print(df[df.index.duplicated(keep=False)])\n",
        "\n",
        "  # reduce dataframe columns to only the average\n",
        "  df = df[[variable_name]]\n",
        "\n",
        "  print(df.index)\n",
        "  dfs.append(df)\n",
        "\n",
        "# concat dataframes into one\n",
        "big_df = pd.concat(dfs, axis=1)\n",
        "print(big_df.shape)\n",
        "print(big_df.head())"
      ],
      "metadata": {
        "id": "En995D-zR1TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#see how many nan values exist now\n",
        "print(big_df.isnull().sum())\n",
        "\n",
        "# delete rows from beginning and end that contain NaN values (since date range for each variable is not the same)\n",
        "\n",
        "# drop nan rows from beginning\n",
        "while(big_df.iloc[0].isnull().values.any() == True):\n",
        "  big_df.drop(index=big_df.index[0], axis=0, inplace=True)\n",
        "\n",
        "#drop nan rows from end\n",
        "while(big_df.iloc[-1].isnull().values.any() == True):\n",
        "  big_df.drop(index=big_df.index[-1], axis=0, inplace=True)\n",
        "\n",
        "#see how many nan values exist after slicing the beginning and end of dataframe\n",
        "print(big_df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "bYdWdmsQ-JRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Current Data"
      ],
      "metadata": {
        "id": "rqHQ7TuJD9kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_df(df):\n",
        "  plot_cols = df.columns\n",
        "  fig,ax = plt.subplots(len(plot_cols), figsize=(20,40), sharex=True)\n",
        "  df.plot(subplots=True, legend=False, ax=ax)\n",
        "  for a in range(len(ax)): \n",
        "      ax[a].set_ylabel(plot_cols[a])\n",
        "  ax[-1].set_xlabel('')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "plot_df(big_df)"
      ],
      "metadata": {
        "id": "hlKX8eMQD8UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove unhelpful variables"
      ],
      "metadata": {
        "id": "7OVWnglmFeDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# based on the graphs, remove latitude and longitude.\n",
        "# Also remove AQI since this number is determined based on pollutant levels (which we already have)\n",
        "big_df.drop(['AQI', 'Latitude', 'Longitude', 'Elevation'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "jLaQULuBFhv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmented Dickey-Fuller Test"
      ],
      "metadata": {
        "id": "ocx7kE76cvXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test for stationarity, difference if seasonality exists\n",
        "# https://michael-fuchs-python.netlify.app/2020/10/29/time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables/#stationarity\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "def Augmented_Dickey_Fuller_Test_func(timeseries , column_name):\n",
        "    '''\n",
        "    Calculates statistical values whether the available data are stationary or not \n",
        "    \n",
        "    Args:\n",
        "        series (float64): Values of the column for which stationarity is to be checked, numpy array of floats \n",
        "        column_name (str): Name of the column for which stationarity is to be checked\n",
        "    \n",
        "    Returns:\n",
        "        p-value that indicates whether the data are stationary or not\n",
        "    ''' \n",
        "    print (f'Results of Dickey-Fuller Test for column: {column_name}')\n",
        "    adfTest = adfuller(timeseries, autolag='AIC')   # why AIC vs BIC, t-stat, etc.?\n",
        "    dfResults = pd.Series(adfTest[0:4], index=['ADF Test Statistic','P-Value','# Lags Used','# Observations Used'])\n",
        "    for key, value in adfTest[4].items():\n",
        "       dfResults['Critical Value (%s)'%key] = value\n",
        "    print (dfResults)\n",
        "    if adfTest[1] <= 0.05:\n",
        "        print()\n",
        "        print(\"Conclusion:\")\n",
        "        print(\"Reject the null hypothesis\")\n",
        "        print('\\033[92m' + \"Data is stationary\" + '\\033[0m')\n",
        "    else:\n",
        "        print()\n",
        "        print(\"Conclusion:\")\n",
        "        print(\"Fail to reject the null hypothesis\")\n",
        "        print('\\033[91m' + \"Data is non-stationary\" + '\\033[0m')\n",
        "\n",
        "\n",
        "# Check each column for seasonality\n",
        "for name, column in big_df.iteritems():\n",
        "    Augmented_Dickey_Fuller_Test_func(big_df[name],name)\n",
        "    print('\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "uqOt2BEacy9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since not all variables are stationary, we must perform co-integration test (apparently I could have skipped the dickey-fuller test?)"
      ],
      "metadata": {
        "id": "xJq7OyEWkVVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.vector_ar.vecm import coint_johansen   #can't use on more than 12 variables\n",
        "\n",
        "johansenResults = coint_johansen(big_df.iloc[:,:12],-1,1)\n",
        "\n",
        "print(\"Trace Stat:\")\n",
        "print(johansenResults.trace_stat)\n",
        "print(\"\\nTrace Stat Crit Vals:\")\n",
        "print(johansenResults.trace_stat_crit_vals)\n",
        "print(\"\\nMax Eig stat:\")\n",
        "print(johansenResults.max_eig_stat)\n",
        "print(\"\\nMax Eig Stat Crit Vals:\")\n",
        "print(johansenResults.max_eig_stat_crit_vals)\n"
      ],
      "metadata": {
        "id": "Rwxm91m_lE4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference the non-stationary variables"
      ],
      "metadata": {
        "id": "jE8UQDxPgsTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_stationary_columns = [\"PM10\", \"CO2\", \"CO\", \"Temperature\"]\n",
        "\n",
        "# visualize cols before\n",
        "plot_df(big_df[non_stationary_columns])"
      ],
      "metadata": {
        "id": "0ou1Qx-mgr9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# difference non-stationary variables\n",
        "transformed_big_df = big_df.copy()\n",
        "transformed_big_df[non_stationary_columns] = transformed_big_df[non_stationary_columns].apply(lambda x: x.diff())\n",
        "\n",
        "# drop nan rows from beginning, differencing produces a NaN for first value\n",
        "while(transformed_big_df.iloc[0].isnull().values.any() == True):\n",
        "  transformed_big_df.drop(index=transformed_big_df.index[0], axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "SAXnfYUlkhIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize after\n",
        "plot_df(transformed_big_df[non_stationary_columns])"
      ],
      "metadata": {
        "id": "SiqwPvw4kr_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the adf test to check for stationary data\n",
        "\n",
        "# Check each column for seasonality\n",
        "for name, column in transformed_big_df.iteritems():\n",
        "    Augmented_Dickey_Fuller_Test_func(transformed_big_df[name],name)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "sMN9Suq0iqBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Granger Causality Test"
      ],
      "metadata": {
        "id": "GkQv3TOCCa78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://blogs.sap.com/2021/05/06/a-multivariate-time-series-modeling-and-forecasting-guide-with-python-machine-learning-client-for-sap-hana/\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import numpy as np\n",
        "\n",
        "maxlag_ = 20\n",
        "variables = transformed_big_df.columns\n",
        "matrix = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "for col in matrix.columns:\n",
        "    for row in matrix.index:\n",
        "        test_result = grangercausalitytests(transformed_big_df[[row, col]], maxlag=maxlag_, verbose=False)            \n",
        "        p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag_)]            \n",
        "        min_p_value = np.min(p_values)\n",
        "        matrix.loc[row, col] = min_p_value\n",
        "matrix.columns = [var + '_x' for var in variables]\n",
        "matrix.index = [var + '_y' for var in variables]\n",
        "print(matrix)"
      ],
      "metadata": {
        "id": "aRdAPOdtCDoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the train and validation set\n",
        "train_size = 0.7\n",
        "\n",
        "transformed_train_df = transformed_big_df[:int(train_size*(len(transformed_big_df)))]\n",
        "train_df = big_df[:int(train_size*(len(big_df)))]\n",
        "print(len(train_df))\n",
        "valid_df = big_df[int(train_size*(len(big_df))):]\n",
        "print(len(valid_df))"
      ],
      "metadata": {
        "id": "d8NPPw6fmVqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get best AR terms"
      ],
      "metadata": {
        "id": "M9pvWUrtpQ4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.statespace.varmax import VARMAX\n",
        "\n",
        "## MODEL RUN NUMBER ##\n",
        "RUN_ID = 1\n",
        "\n",
        "# used to select best AIC lag order\n",
        "model = VAR(transformed_big_df)\n",
        "sorted_order=model.select_order(maxlags=20)\n",
        "print(sorted_order.summary())"
      ],
      "metadata": {
        "id": "EaFcdyZHpSpo",
        "outputId": "14878c9e-caea-4ea3-c5e8-05e62ea2befe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/base/tsa_model.py:524: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
            "  warnings.warn('No frequency information was'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " VAR Order Selection (* highlights the minimums)  \n",
            "==================================================\n",
            "       AIC         BIC         FPE         HQIC   \n",
            "--------------------------------------------------\n",
            "0        37.55       37.63   2.023e+16       37.58\n",
            "1        5.546       6.950       256.1       6.081\n",
            "2        3.225      5.952*       25.17      4.265*\n",
            "3       3.015*       7.063      20.42*       4.558\n",
            "4        3.098       8.468       22.22       5.145\n",
            "5        3.091       9.784       22.16       5.643\n",
            "6        3.108       11.12       22.63       6.163\n",
            "7        3.244       12.58       26.13       6.804\n",
            "8        3.342       14.00       29.08       7.405\n",
            "9        3.508       15.49       34.74       8.075\n",
            "10       3.701       17.00       42.79       8.773\n",
            "11       3.769       18.39       46.61       9.344\n",
            "12       3.906       19.85       54.68       9.986\n",
            "13       4.026       21.29       63.28       10.61\n",
            "14       4.169       22.76       75.27       11.26\n",
            "15       4.315       24.23       90.36       11.91\n",
            "16       4.420       25.65       104.6       12.52\n",
            "17       4.578       27.13       128.4       13.18\n",
            "18       4.609       28.49       139.8       13.71\n",
            "19       4.742       29.94       169.6       14.35\n",
            "20       4.851       31.37       202.6       14.96\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PqWlmoaBrwCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the non differenced df since VARMAX can do its own automatic differencing\n",
        "# second order is 0 since we're not using moving average here (MAX part of VARMAX)\n",
        "var_model = VARMAX(train_df, order=(3,0), enforce_stationarity=True)\n",
        "fitted_model = var_model.fit(disp=False)\n",
        "\n",
        "#save model for future reference\n",
        "fitted_model.save('/content/drive/MyDrive/Air_Pollution_Models/{}_run.png'.format(RUN_ID))\n",
        "print(fitted_model.summary())"
      ],
      "metadata": {
        "id": "BQvnDW4spgfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_interval = 60 #change depending on data interval\n",
        "n_day_forecast = int(1440 / min_interval)\n",
        "print(n_day_forecast)\n",
        "n_week_forecast = int(n_day_forecast*7)\n",
        "n_2week_forecast = int(n_week_forecast*2)\n",
        "n_month_forecast = int(n_2week_forecast*2)\n",
        "predict_day = fitted_model.get_prediction(start=len(train_df), end=len(train_df) + n_day_forecast - 1)\n",
        "predict_week = fitted_model.get_prediction(start=len(train_df), end=len(train_df) + n_week_forecast - 1)\n",
        "predict_2week = fitted_model.get_prediction(start=len(train_df), end=len(train_df) + n_2week_forecast - 1)\n",
        "predict_month = fitted_model.get_prediction(start=len(train_df), end=len(train_df) + n_month_forecast - 1)\n",
        "\n",
        "predictions_day=predict_day.predicted_mean.add_suffix('_Prediction')\n",
        "predictions_week=predict_week.predicted_mean.add_suffix('_Prediction')\n",
        "predictions_2week=predict_2week.predicted_mean.add_suffix('_Prediction')\n",
        "predictions_month=predict_month.predicted_mean.add_suffix('_Prediction')"
      ],
      "metadata": {
        "id": "SBwdIM7q0Hk-",
        "outputId": "66defd8d-06c2-40db-e056-481f92785e27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_vs_pred_day_df = pd.concat([valid_df.iloc[:n_day_forecast], predictions_day], axis=1)\n",
        "valid_vs_pred_week_df = pd.concat([valid_df.iloc[:n_week_forecast], predictions_week], axis=1)\n",
        "valid_vs_pred_2week_df = pd.concat([valid_df.iloc[:n_2week_forecast], predictions_2week], axis=1)\n",
        "valid_vs_pred_month_df = pd.concat([valid_df.iloc[:n_month_forecast], predictions_month], axis=1)"
      ],
      "metadata": {
        "id": "KEoAYSaK6QiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import acf\n",
        "def forecast_accuracy(forecast, actual):\n",
        "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE\n",
        "    me = np.mean(forecast - actual)             # ME\n",
        "    mae = np.mean(np.abs(forecast - actual))    # MAE\n",
        "    mpe = np.mean((forecast - actual)/actual)   # MPE\n",
        "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
        "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
        "    mins = np.amin(np.hstack([forecast[:,None], \n",
        "                              actual[:,None]]), axis=1)\n",
        "    maxs = np.amax(np.hstack([forecast[:,None], \n",
        "                              actual[:,None]]), axis=1)\n",
        "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
        "    \n",
        "    return({'mape':mape, 'me':me, 'mae': mae, \n",
        "            'mpe': mpe, 'rmse':rmse, 'corr':corr, 'minmax':minmax})\n"
      ],
      "metadata": {
        "id": "dWNPaRwuMzNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_columns = [\"PM1\", \"PM2\", \"PM10\", \"CO\", \"CO2\", \"O3\", \"NO\", \"NO2\"]\n",
        "def adjust(val, length= 6): \n",
        "  return str(val).ljust(length)\n",
        "\n",
        "metrics_df = pd.DataFrame()\n",
        "\n",
        "for pred_range_and_df in [(\"Day\", valid_vs_pred_day_df), (\"Week\", valid_vs_pred_week_df), (\"2_weeks\", valid_vs_pred_2week_df), (\"Month\", valid_vs_pred_month_df)]:\n",
        "  for col in pred_columns:\n",
        "    print('\\nForecast accuracy of ' + col)\n",
        "    pred_range, pred_df = pred_range_and_df\n",
        "    accuracy_prod = forecast_accuracy(pred_df[col + '_Prediction'].values, pred_df[col])\n",
        "    for k, v in accuracy_prod.items():\n",
        "      print(adjust(k), ': ', round(v,4))\n",
        "      metrics_df.at[pred_range, col, k] = v\n",
        "\n",
        "print(metrics_df)"
      ],
      "metadata": {
        "id": "9vxC3qJ7MlLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "pred_columns = [\"PM1\", \"PM2\", \"PM10\", \"CO\", \"CO2\", \"O3\", \"NO\", \"NO2\"]\n",
        "fig, axes = plt.subplots(len(pred_columns), 2, figsize=(24, 50))\n",
        "i = 0\n",
        "for column in pred_columns:\n",
        "  valid_vs_pred_day_df.plot(y=[column, column + \"_Prediction\"], ax=axes[i,0])\n",
        "  valid_vs_pred_week_df.plot(y=[column, column + \"_Prediction\"],ax=axes[i,1])\n",
        "  i = i + 1\n",
        "  \n",
        "plt.savefig('/content/drive/MyDrive/Air_Pollution_Prediction_Figures/{}_run.png'.format(RUN_ID))"
      ],
      "metadata": {
        "id": "OH35lsAM6j6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AR_Term_value_VAR = best_values_VAR['AR_Term'].iloc[0]\n",
        "\n",
        "print(\"AR_Term_value_VAR: \", AR_Term_value_VAR)"
      ],
      "metadata": {
        "id": "ncHfiAmgp6bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hana_ml.algorithms.pal.tsa.vector_arima import VectorARIMA\n",
        "\n",
        "vectorArima1 = VectorARIMA(order=(-1, 2, -1), model_type = 'VARMA', search_method='grid_search', output_fitted=True, max_p=5, max_q=5)\n",
        "vectorArima1.fit(data=train)"
      ],
      "metadata": {
        "id": "A5a1S94coYT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "\n",
        "model = VAR(endog=train)\n",
        "model_fit = model.fit()"
      ],
      "metadata": {
        "id": "MAuguOsUnFHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make prediction on validation\n",
        "prediction = model_fit.forecast(model_fit.y, steps=len(valid))"
      ],
      "metadata": {
        "id": "gzveo1a1nbaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting predictions to dataframe\n",
        "pred = pd.DataFrame(index=range(0,len(prediction)),columns=[cols])\n",
        "for j in range(0,13):\n",
        "    for i in range(0, len(prediction)):\n",
        "       pred.iloc[i][j] = prediction[i][j]"
      ],
      "metadata": {
        "id": "FWkxKcZ9neI6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}